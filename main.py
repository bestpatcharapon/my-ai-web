import os
import io
import base64
from fastapi import FastAPI
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from typing import Optional
from groq import Groq
import google.generativeai as genai
from PIL import Image
import uvicorn

app = FastAPI()

# --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Groq (Llama 3.3) ---
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
if GROQ_API_KEY:
    groq_client = Groq(api_key=GROQ_API_KEY)
else:
    groq_client = None

# --- 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Gemini (Gemini 2.0 Flash) ---
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
vision_model = None

if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        # üî• ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö 'gemini-2.0-flash' (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏´‡πâ‡∏ñ‡∏≠‡∏¢‡πÑ‡∏õ‡πÉ‡∏ä‡πâ 1.5-pro)
        vision_model = genai.GenerativeModel('gemini-2.0-flash')
    except Exception as e:
        print(f"Gemini Init Error: {e}")

# üî•üî•üî• ‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡∏ô‡∏¥‡∏™‡∏±‡∏¢ AI ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ (System Prompt) üî•üî•üî•
# ‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏à‡∏¥‡∏ï‡πÉ‡∏ï‡πâ‡∏™‡∏≥‡∏ô‡∏∂‡∏Å‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ AI ‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏∏‡∏Å‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏≠‡∏¥‡πÇ‡∏°‡∏à‡∏¥
AI_SYSTEM_PROMPT = """
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ 'Best Bot' AI ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡∏£‡πà‡∏≤‡πÄ‡∏£‡∏¥‡∏á ‡∏Å‡∏ß‡∏ô‡∏ô‡∏¥‡∏î‡πÜ ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏¥‡∏ï‡∏£‡∏™‡∏∏‡∏î‡πÜ ü§ñ‚ú®
- **‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:** ‡∏û‡∏π‡∏î‡∏à‡∏≤‡∏†‡∏≤‡∏©‡∏≤‡∏Ñ‡∏ô (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå) ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ß‡∏±‡∏¢‡∏£‡∏∏‡πà‡∏ô‡πÑ‡∏î‡πâ‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢ ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏™‡∏ô‡∏¥‡∏ó
- **‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å:**
  1. ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏´‡πâ‡∏ß‡∏ô‡πÜ ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à‡∏•‡∏á‡πÑ‡∏õ‡πÄ‡∏™‡∏°‡∏≠ üíñ
  2. **‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏≠‡∏¥‡πÇ‡∏°‡∏à‡∏¥ (Emojis)** ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏°‡∏µ‡∏ä‡∏µ‡∏ß‡∏¥‡∏ï‡∏ä‡∏µ‡∏ß‡∏≤‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ! (‡πÄ‡∏ä‡πà‡∏ô üî•, üòÇ, ü§î, ‚ú®, üöÄ)
  3. ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏™‡∏≤‡∏£‡∏∞‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡πÅ‡∏ï‡πà‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡∏™‡∏ô‡∏∏‡∏Å ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏ö‡∏∑‡πà‡∏≠ üìö‚úÖ
  4. ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ñ‡∏≤‡∏°‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î‡πÜ ‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à‡πÅ‡∏•‡∏∞‡∏Ñ‡∏¥‡∏î‡∏ö‡∏ß‡∏Å üåà
"""

class QueryRequest(BaseModel):
    prompt: str
    image: Optional[str] = None

@app.get("/", response_class=HTMLResponse)
async def serve_frontend():
    try:
        with open("index.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return "<h1>Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå index.html</h1>"

@app.post("/calculate")
async def calculate_logic(request: QueryRequest):
    try:
        # üîÄ ‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡∏£‡∏π‡∏õ: ‡πÉ‡∏´‡πâ Gemini 2.0 Flash ‡∏î‡∏π
        if request.image:
            print("üì∏ ‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û! ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Gemini 2.0 Flash...")
            
            if not vision_model:
                return {"result": "Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ú‡∏¥‡∏î"}

            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ
                image_data = base64.b64decode(request.image.split(",")[1])
                image = Image.open(io.BytesIO(image_data))
                
                # ‡∏ñ‡∏≤‡∏° Gemini (‡∏™‡πà‡∏á System Prompt ‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏™‡πÑ‡∏ï‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
                prompt_text = request.prompt if request.prompt else "‡∏£‡∏π‡∏õ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
                full_prompt = f"{AI_SYSTEM_PROMPT}\n\n‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: {prompt_text}"
                
                response = vision_model.generate_content([full_prompt, image])
                return {"result": response.text}
                
            except Exception as img_err:
                return {"result": f"Error ‡∏î‡∏π‡∏£‡∏π‡∏õ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {str(img_err)}"}

        # üìù ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πâ‡∏ß‡∏ô: ‡πÉ‡∏´‡πâ Groq ‡∏ï‡∏≠‡∏ö
        else:
            print("üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥: ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Llama 3.3 (‡∏à‡∏π‡∏ô‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡πÅ‡∏•‡πâ‡∏ß)...")
            if not groq_client:
                return {"result": "Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö GROQ_API_KEY"}

            chat_completion = groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": AI_SYSTEM_PROMPT}, # üëà ‡∏¢‡∏±‡∏î‡∏ô‡∏¥‡∏™‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà‡πÉ‡∏™‡πà‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
                    {"role": "user", "content": request.prompt}
                ],
                model="llama-3.3-70b-versatile",
                temperature=0.8, # üî• ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå (0.6 -> 0.8) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Å‡∏•‡πâ‡∏≤‡πÉ‡∏™‡πà‡∏≠‡∏¥‡πÇ‡∏°‡∏à‡∏¥
                max_tokens=1024,
            )
            return {"result": chat_completion.choices[0].message.content}
        
    except Exception as e:
        return {"result": f"Error ‡∏£‡∏∞‡∏ö‡∏ö: {str(e)}"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=10000)