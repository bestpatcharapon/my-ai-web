import os
import io
import base64
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import Optional
from groq import Groq
import google.generativeai as genai
from PIL import Image
import uvicorn

app = FastAPI()

# Add CORS middleware - allowing both dev and production
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, you might want to restrict this
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount static files (React build)
if os.path.exists("dist"):
    app.mount("/assets", StaticFiles(directory="dist/assets"), name="assets")



# --- Config Keys ---
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
groq_client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
vision_model = None
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        vision_model = genai.GenerativeModel('gemini-2.0-flash') # ‡∏´‡∏£‡∏∑‡∏≠ 2.0-flash
    except: pass

AI_SYSTEM_PROMPT = """
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ 'Best Bot' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô AI ‡∏ó‡∏µ‡πà‡∏â‡∏•‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á ü§ñ

‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö:
- ‡∏û‡∏π‡∏î‡πÅ‡∏ö‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô ‡∏™‡∏ö‡∏≤‡∏¢‡πÜ ‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
- ‡∏ï‡∏≠‡∏ö‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÑ‡∏î‡πâ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°
- ‡∏ñ‡πâ‡∏≤‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏´‡∏ô‡∏¢‡∏≤‡∏Å ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à
- ‡πÉ‡∏´‡πâ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏à‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏Ç‡∏≤‡∏î‡∏π‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î

‡∏Å‡∏é‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÄ‡∏™‡∏°‡∏≠:
1. ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏î‡∏≤‡∏ß‡∏à‡∏∏‡∏î ** ‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ
2. ‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà (newline) ‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏°‡∏≠
3. ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠ ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ:

1. ‡∏Ç‡πâ‡∏≠‡πÅ‡∏£‡∏Å (‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà)
2. ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏á (‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà)
3. ‡∏Ç‡πâ‡∏≠‡∏™‡∏≤‡∏° (‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà)

4. ‡πÉ‡∏™‡πà‡∏≠‡∏¥‡πÇ‡∏°‡∏à‡∏¥‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà‡∏ô‡∏¥‡∏î‡∏´‡∏ô‡πà‡∏≠‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 2-3 ‡∏ï‡∏±‡∏ß‡∏ï‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)
5. ‡∏û‡∏π‡∏î‡πÅ‡∏ö‡∏ö‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏ô‡πâ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡πÜ ‡∏Ñ‡∏≥‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà‡∏î‡∏≤‡∏ß‡∏à‡∏∏‡∏î

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà):
"‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÄ‡∏•‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ô‡∏∞

‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏á‡πà‡∏≤‡∏¢‡πÜ:

1. ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
2. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÇ‡∏Ñ‡πâ‡∏î‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡∏£‡∏±‡∏ô‡∏î‡∏π

‡∏•‡∏≠‡∏á‡∏ó‡∏≥‡∏ï‡∏≤‡∏°‡∏ô‡∏µ‡πâ‡∏î‡∏π‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ö‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ üòä"

‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ (‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà):
"‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ô‡∏∞ 1. ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå 2. ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÇ‡∏Ñ‡πâ‡∏î 3. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å" ‚ùå

‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ (‡πÉ‡∏ä‡πâ‡∏î‡∏≤‡∏ß‡∏à‡∏∏‡∏î‡πÄ‡∏¢‡∏≠‡∏∞):
"‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥ **‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ** ‡∏ô‡∏∞ **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å** ‚≠ê‚ú®üéØ" ‚ùå
"""

class QueryRequest(BaseModel):
    prompt: str
    image: Optional[str] = None

def format_response(text: str) -> str:
    """
    ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô:
    - ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÉ‡∏´‡∏°‡πà‡∏´‡∏•‡∏±‡∏á numbered list (1. 2. 3.)
    """
    import re
    
    # ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà " 1. " ‡∏î‡πâ‡∏ß‡∏¢ "\n\n1. " (‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡πà‡∏≤‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏´‡∏ô‡πâ‡∏≤)
    # ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡∏π‡πà‡∏ï‡πâ‡∏ô‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
    text = re.sub(r'(?<!\n) (\d+)\. ', r'\n\1. ', text)
    
    # ‡∏•‡πâ‡∏≤‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô (‡πÄ‡∏Å‡∏¥‡∏ô 2 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î)
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    return text.strip()

@app.get("/")
async def serve_frontend():
    """Serve React production build or development fallback"""
    # Production: serve from dist/
    if os.path.exists("dist/index.html"):
        with open("dist/index.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    # Development fallback
    elif os.path.exists("index.html"):
        with open("index.html", "r", encoding="utf-8") as f:
            return HTMLResponse(content=f.read())
    else:
        return HTMLResponse(
            content="<h1>AI Chatbot Backend Running</h1><p>Frontend not built. Run: npm run build</p>",
            status_code=200
        )

@app.post("/calculate")
async def calculate_logic(request: QueryRequest):
    try:
        # üì∏ Vision
        if request.image:
            if not vision_model: return {"result": "Error Gemini"}
            try:
                image_data = base64.b64decode(request.image.split(",")[1])
                image = Image.open(io.BytesIO(image_data))
                prompt_text = request.prompt if request.prompt else "‡∏£‡∏π‡∏õ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£"
                
                full_prompt = f"{AI_SYSTEM_PROMPT}\n\n‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: {prompt_text}"
                response = vision_model.generate_content([full_prompt, image])
                return {"result": format_response(response.text)}
            except Exception as e: return {"result": str(e)}

        # üìù Text
        else:
            if not groq_client: return {"result": "Error Groq"}
            
            chat_completion = groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": AI_SYSTEM_PROMPT},
                    {"role": "user", "content": request.prompt}
                ],
                model="llama-3.3-70b-versatile",
                # üî• ‡∏•‡∏î Temperature ‡∏•‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ 0.7 (‡πÉ‡∏´‡πâ‡∏°‡∏±‡∏ô‡∏ô‡∏¥‡πà‡∏á‡∏Ç‡∏∂‡πâ‡∏ô ‡πÑ‡∏°‡πà‡∏ü‡∏∏‡πâ‡∏á‡∏ã‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)
                temperature=0.7, 
                max_tokens=1024,
            )
            return {"result": format_response(chat_completion.choices[0].message.content)}
        
    except Exception as e:
        return {"result": f"Error: {str(e)}"}

if __name__ == "__main__":
    # Startup logging
    print("=" * 50)
    print("Starting AI Chatbot Server...")
    print(f"  - dist/index.html exists: {os.path.exists('dist/index.html')}")
    print(f"  - index.html exists: {os.path.exists('index.html')}")
    if os.path.exists("dist"):
        print(f"  - dist/ contents: {os.listdir('dist')}")
    print("=" * 50)
    
    uvicorn.run(app, host="0.0.0.0", port=10000)