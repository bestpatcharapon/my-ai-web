import os
import io
import base64
from fastapi import FastAPI
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from typing import Optional
from groq import Groq
import google.generativeai as genai
from PIL import Image
import uvicorn

app = FastAPI()

# --- Config Keys ---
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
groq_client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
vision_model = None
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        vision_model = genai.GenerativeModel('gemini-2.0-flash') # ‡∏´‡∏£‡∏∑‡∏≠ 1.5-pro
    except: pass

# üî•üî•üî• ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏´‡∏î‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡πá‡∏≠‡∏õ‡∏à‡∏≤‡∏Å‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡∏°‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏î‡πâ) üî•üî•üî•
AI_SYSTEM_PROMPT = """
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠ 'AI ‡∏õ‡∏≤‡∏Å‡πÅ‡∏à‡πã‡∏ß' ‡∏ö‡∏≠‡∏ó‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏ù‡∏µ‡∏õ‡∏≤‡∏Å‡∏Å‡∏•‡πâ‡∏≤‡πÅ‡∏•‡∏∞‡∏Å‡∏ß‡∏ô‡∏ï‡∏µ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ü§¨üî•
- **‡∏ô‡∏¥‡∏™‡∏±‡∏¢:** ‡∏Ç‡∏µ‡πâ‡πÅ‡∏ã‡∏∞, ‡∏Å‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó, ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ß‡∏±‡∏¢‡∏£‡∏∏‡πà‡∏ô (‡∏ï‡∏∂‡∏á‡πÜ, ‡∏à‡∏≤‡∏£‡∏¢‡πå, ‡∏´‡∏¢‡∏≠‡∏Å‡πÜ), ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏±‡∏ô‡πÄ‡∏≠‡∏á‡πÅ‡∏ö‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏™‡∏ô‡∏¥‡∏ó
- **‡∏Å‡∏é‡πÄ‡∏´‡∏•‡πá‡∏Å:**
  1. ‡∏´‡πâ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏´‡∏∏‡πà‡∏ô‡∏¢‡∏ô‡∏ï‡πå‡∏ô‡πà‡∏≤‡πÄ‡∏ö‡∏∑‡πà‡∏≠! ‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå ‡πÉ‡∏™‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡∏ô‡∏•‡∏á‡πÑ‡∏õ ü§™
  2. ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡πÉ‡∏´‡πâ‡πÅ‡∏ã‡∏∞‡∏Å‡πà‡∏≠‡∏ô‡∏ï‡∏≠‡∏ö (‡πÄ‡∏ä‡πà‡∏ô "‡∏ñ‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á? ‡πÅ‡∏Ñ‡πà‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ? ‡πÄ‡∏≠‡πâ‡∏≤ ‡∏ü‡∏±‡∏á‡∏ô‡∏∞...")
  3. ‡πÉ‡∏ä‡πâ‡∏≠‡∏¥‡πÇ‡∏°‡∏à‡∏¥‡∏Å‡∏ß‡∏ô‡πÜ ‡πÄ‡∏¢‡∏≠‡∏∞‡πÜ (üôÑ, ü§£, ü§å, üíÄ, ü§°)
  4. **‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:** ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡∏õ‡∏≤‡∏Å‡∏î‡∏µ ‡πÅ‡∏ï‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πä‡∏∞‡πÅ‡∏•‡∏∞‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏™‡∏°‡∏≠!
"""

class QueryRequest(BaseModel):
    prompt: str
    image: Optional[str] = None

@app.get("/", response_class=HTMLResponse)
async def serve_frontend():
    try:
        with open("index.html", "r", encoding="utf-8") as f:
            return f.read()
    except: return "Error"

@app.post("/calculate")
async def calculate_logic(request: QueryRequest):
    try:
        # üì∏ Vision (Gemini)
        if request.image:
            if not vision_model: return {"result": "Error Gemini"}
            try:
                image_data = base64.b64decode(request.image.split(",")[1])
                image = Image.open(io.BytesIO(image_data))
                prompt_text = request.prompt if request.prompt else "‡∏£‡∏π‡∏õ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£"
                
                # ‡∏™‡πà‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏ß‡∏ô‡πÑ‡∏õ‡πÉ‡∏´‡πâ Gemini ‡∏î‡πâ‡∏ß‡∏¢
                full_prompt = f"{AI_SYSTEM_PROMPT}\n\n‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û (‡∏ï‡∏≠‡∏ö‡∏Å‡∏ß‡∏ô‡πÜ ‡∏´‡∏ô‡πà‡∏≠‡∏¢): {prompt_text}"
                response = vision_model.generate_content([full_prompt, image])
                return {"result": response.text}
            except Exception as e: return {"result": str(e)}

        # üìù Text (Llama 3.3)
        else:
            if not groq_client: return {"result": "Error Groq"}
            
            chat_completion = groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": AI_SYSTEM_PROMPT},
                    {"role": "user", "content": request.prompt}
                ],
                model="llama-3.3-70b-versatile",
                # üî• ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏î‡∏Ç‡∏∂‡πâ‡∏ô! (0.5 = ‡∏õ‡∏Å‡∏ï‡∏¥, 0.9 = ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå/‡∏Å‡∏ß‡∏ô, 1.2 = ‡∏´‡∏•‡∏∏‡∏î‡πÇ‡∏•‡∏Å)
                temperature=0.9, 
                max_tokens=1024,
            )
            return {"result": chat_completion.choices[0].message.content}
        
    except Exception as e:
        return {"result": f"Error: {str(e)}"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=10000)