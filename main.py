import os
import io
import base64
from fastapi import FastAPI
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from typing import Optional
from groq import Groq
import google.generativeai as genai
from PIL import Image
import uvicorn

app = FastAPI()

# --- API Keys Configuration ---
# ‚úÖ ‡πÉ‡∏ä‡πâ Environment Variables ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô Render: Settings > Environment > Add Variable
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ API Keys ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
if not GROQ_API_KEY or not GEMINI_API_KEY:
    print("‚ö†Ô∏è Warning: API Keys not found! Please set environment variables:")
    print("   - GROQ_API_KEY")
    print("   - GEMINI_API_KEY")

# ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö AI Services
groq_client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    vision_model = genai.GenerativeModel('gemini-2.0-flash-exp')
else:
    vision_model = None

class QueryRequest(BaseModel):
    prompt: str
    image: Optional[str] = None

# ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô (‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡πÑ‡∏ü‡∏•‡πå HTML)
@app.get("/", response_class=HTMLResponse)
async def serve_frontend():
    try:
        with open("index.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return "<h1>Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå index.html (‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡∏∂‡πâ‡∏ô GitHub ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö)</h1>"

# ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ö‡πâ‡∏≤‡∏ô (‡∏™‡∏°‡∏≠‡∏á AI ‡∏™‡∏±‡∏ö‡∏£‡∏≤‡∏á)
@app.post("/calculate")
async def calculate_logic(request: QueryRequest):
    try:
        # üîÄ ‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û: ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Gemini 2.0 ‡∏î‡∏π
        if request.image:
            print("üì∏ ‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û! ‡πÉ‡∏´‡πâ Gemini 2.0 Flash ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏π...")
            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ
                image_data = base64.b64decode(request.image.split(",")[1])
                image = Image.open(io.BytesIO(image_data))
                
                # ‡∏ñ‡∏≤‡∏° Gemini
                prompt_text = request.prompt if request.prompt else "‡∏£‡∏π‡∏õ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
                response = vision_model.generate_content([prompt_text, image])
                return {"result": response.text}
                
            except Exception as img_err:
                return {"result": f"Error ‡∏î‡∏π‡∏£‡∏π‡∏õ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {str(img_err)}"}

        # üìù ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πâ‡∏ß‡∏ô: ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Groq (Llama 3.3) ‡∏ï‡∏≠‡∏ö
        else:
            print("üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥: ‡πÉ‡∏´‡πâ Llama 3.3 (70B) ‡∏ï‡∏≠‡∏ö...")
            chat_completion = groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant fluent in Thai."},
                    {"role": "user", "content": request.prompt}
                ],
                model="llama-3.3-70b-versatile",
                temperature=0.6,
                max_tokens=1024,
            )
            return {"result": chat_completion.choices[0].message.content}
        
    except Exception as e:
        return {"result": f"Error ‡∏£‡∏∞‡∏ö‡∏ö: {str(e)}"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=10000)