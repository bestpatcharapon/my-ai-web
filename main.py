import os
import io
import base64
from fastapi import FastAPI
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from typing import Optional
from groq import Groq
import google.generativeai as genai
from PIL import Image
import uvicorn

app = FastAPI()

# --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Groq (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏¢‡∏õ‡∏Å‡∏ï‡∏¥ - Llama 3.3) ---
# ‡∏î‡∏∂‡∏á Key ‡∏à‡∏≤‡∏Å Environment Variable
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
groq_client = Groq(api_key=GROQ_API_KEY)

# --- 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Gemini (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏π‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û) ---
# ‡∏î‡∏∂‡∏á Key ‡∏à‡∏≤‡∏Å Environment Variable
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")

vision_model = None
if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        # üî• ‡πÅ‡∏Å‡πâ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ: ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÉ‡∏ä‡πâ 1.5 Flash (‡∏ï‡∏±‡∏ß‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ ‡πÇ‡∏Ñ‡∏ß‡∏ï‡∏≤‡πÄ‡∏¢‡∏≠‡∏∞)
        vision_model = genai.GenerativeModel('gemini-1.5-flash')
    except Exception as e:
        print(f"Gemini Error: {e}")

class QueryRequest(BaseModel):
    prompt: str
    image: Optional[str] = None

# ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ö‡πâ‡∏≤‡∏ô (‡πÄ‡∏™‡∏¥‡∏£‡πå‡∏ü‡πÑ‡∏ü‡∏•‡πå HTML)
@app.get("/", response_class=HTMLResponse)
async def serve_frontend():
    try:
        with open("index.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return "<h1>Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå index.html (‡∏≠‡∏¢‡πà‡∏≤‡∏•‡∏∑‡∏°‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡∏∂‡πâ‡∏ô GitHub ‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö)</h1>"

# ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏ö‡πâ‡∏≤‡∏ô (‡∏™‡∏°‡∏≠‡∏á AI)
@app.post("/calculate")
async def calculate_logic(request: QueryRequest):
    try:
        # üîÄ ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 1: ‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏™‡πà‡∏á‡∏°‡∏≤ -> ‡πÉ‡∏´‡πâ Gemini ‡∏î‡∏π
        if request.image:
            print("üì∏ ‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û! ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Gemini 1.5 Flash ‡∏î‡∏π...")
            
            if not vision_model:
                return {"result": "Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡πÉ‡∏ô Server (‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î)"}

            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏´‡∏±‡∏™ Base64 ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ
                image_data = base64.b64decode(request.image.split(",")[1])
                image = Image.open(io.BytesIO(image_data))
                
                # ‡∏ñ‡∏≤‡∏° Gemini
                prompt_text = request.prompt if request.prompt else "‡∏£‡∏π‡∏õ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
                response = vision_model.generate_content([prompt_text, image])
                return {"result": response.text}
                
            except Exception as img_err:
                return {"result": f"Error ‡∏î‡∏π‡∏£‡∏π‡∏õ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {str(img_err)}"}

        # üìù ‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏µ‡πà 2: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πâ‡∏ß‡∏ô -> ‡πÉ‡∏´‡πâ Groq (Llama 3.3) ‡∏ï‡∏≠‡∏ö
        else:
            print("üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥: ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Llama 3.3 (70B) ‡∏ï‡∏≠‡∏ö...")
            
            if not GROQ_API_KEY:
                return {"result": "Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö GROQ_API_KEY ‡πÉ‡∏ô Server"}

            chat_completion = groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant fluent in Thai."},
                    {"role": "user", "content": request.prompt}
                ],
                model="llama-3.3-70b-versatile",
                temperature=0.6,
                max_tokens=1024,
            )
            return {"result": chat_completion.choices[0].message.content}
        
    except Exception as e:
        return {"result": f"Error ‡∏£‡∏∞‡∏ö‡∏ö: {str(e)}"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=10000)