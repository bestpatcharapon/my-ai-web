import os
import io
import base64
from fastapi import FastAPI
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from typing import Optional
from groq import Groq
import google.generativeai as genai
from PIL import Image
import uvicorn

app = FastAPI()

# --- 1. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Groq (Llama 3.3) ---
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")
if GROQ_API_KEY:
    groq_client = Groq(api_key=GROQ_API_KEY)
else:
    groq_client = None

# --- 2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Gemini (Gemini 2.0 Flash) ---
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
vision_model = None

if GEMINI_API_KEY:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        # üî• ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ‡∏Ñ‡∏£‡∏±‡∏ö 'gemini-2.0-flash' (‡∏ö‡∏±‡∏ç‡∏ä‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ)
        vision_model = genai.GenerativeModel('gemini-2.0-flash')
    except Exception as e:
        print(f"Gemini Init Error: {e}")

class QueryRequest(BaseModel):
    prompt: str
    image: Optional[str] = None

@app.get("/", response_class=HTMLResponse)
async def serve_frontend():
    try:
        with open("index.html", "r", encoding="utf-8") as f:
            return f.read()
    except FileNotFoundError:
        return "<h1>Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå index.html</h1>"

@app.post("/calculate")
async def calculate_logic(request: QueryRequest):
    try:
        # üîÄ ‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡∏£‡∏π‡∏õ: ‡πÉ‡∏´‡πâ Gemini 2.0 Flash ‡∏î‡∏π
        if request.image:
            print("üì∏ ‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û! ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Gemini 2.0 Flash...")
            
            if not vision_model:
                return {"result": "Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö GEMINI_API_KEY ‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ú‡∏¥‡∏î"}

            try:
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏π‡∏õ
                image_data = base64.b64decode(request.image.split(",")[1])
                image = Image.open(io.BytesIO(image_data))
                
                # ‡∏ñ‡∏≤‡∏° Gemini
                prompt_text = request.prompt if request.prompt else "‡∏£‡∏π‡∏õ‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢"
                response = vision_model.generate_content([prompt_text, image])
                return {"result": response.text}
                
            except Exception as img_err:
                return {"result": f"Error ‡∏î‡∏π‡∏£‡∏π‡∏õ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {str(img_err)}"}

        # üìù ‡∏Å‡∏£‡∏ì‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πâ‡∏ß‡∏ô: ‡πÉ‡∏´‡πâ Groq ‡∏ï‡∏≠‡∏ö
        else:
            print("üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏Å‡∏ï‡∏¥: ‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ Llama 3.3...")
            if not groq_client:
                return {"result": "Error: ‡πÑ‡∏°‡πà‡∏û‡∏ö GROQ_API_KEY"}

            chat_completion = groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant fluent in Thai."},
                    {"role": "user", "content": request.prompt}
                ],
                model="llama-3.3-70b-versatile",
                temperature=0.6,
                max_tokens=1024,
            )
            return {"result": chat_completion.choices[0].message.content}
        
    except Exception as e:
        return {"result": f"Error ‡∏£‡∏∞‡∏ö‡∏ö: {str(e)}"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=10000)